#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\begin_preamble
\lstdefinelanguage{scala}
{
 morekeywords={abstract,case,catch,class,def,%
    do,else,extends,false,final,finally,%
    for,if,implicit,import,match,mixin,%
    new,null,object,override,package,%
    private,protected,requires,return,sealed,%
    super,this,throw,trait,true,try,%
    type,val,var,while,with,yield},
  otherkeywords={=>,<-,<\%,<:,>:,\#,@},
  sensitive=true,
  morecomment=[l]{//},
  morecomment=[n]{/*}{*/},
  morestring=[b]",
  morestring=[b]',
  morestring=[b]"""
}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1cm
\topmargin 1cm
\rightmargin 1cm
\bottommargin 1.5cm
\headheight 1cm
\headsep 1cm
\footskip 1cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Light Weight Threads and Their Communication Primitives
\end_layout

\begin_layout Author
Tormod Hellen
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Section*
Evaluation note
\end_layout

\begin_layout Standard
I want to be evaluated on both this report and the attached C code.
 
\end_layout

\begin_layout Part
Introduction: The Indeterminism Problem
\begin_inset Index idx
status open

\begin_layout Plain Layout
Indeterminism
\end_layout

\end_inset


\end_layout

\begin_layout Standard
If you've ever written concurrent code, you might have noticed that you
 are pretty bad at writing that code.
 I certainly have.
 The nice thing about conventional code executing from top to bottom is
 that there aren't really any surprises.
 As my first programming teacher told me: if something goes wrong the bug
 is further up in the text file (accomodating code flow between functions
 etc.).
 This doesn't really work with concurrent code, you might have discovered.
 Concurrent code usually has the nasty feature of having indeterministic
 execution timing, presenting you with heisenbugs where even reproducing
 the bug is a challenge.
 One solution is to try to hold all the possible ways your threads might
 interact with each other in your head at once, but as the amount of threads
 doing things grow, this becomes unfeasible.
 
\end_layout

\begin_layout Standard
How to counter this onslaught on your sanity? One method is to keep the
 amount of threads working at once down, and using conventional semaphores
 this is the way you usually do it.
 But what if you want to use more threads, what then? Well, that's what
 most of this report is about.
 Some approaches hide and automatically recover from your faults, others
 make them more predictable so they're easier to find.
 
\end_layout

\begin_layout Standard
There are many models of concurrency awailable to the modern engineer.
 The classic semaphore and its relative - lock - are the most native to
 the way our processors look, with shared memory between thread hardware.
 Others, like the actor model, copy a distributed architecture with several
 computers, all with their own memory, processors and network connections.
 Others still, like channel-concurrency, are based on formal or mathematical
 theorems.
 These are the three main means of concurrent communication, though others
 have been introduced.
 We will look at some of them in this report.
\end_layout

\begin_layout Standard
Interestingly, parallel statements and synchronous execution do away with
 the indeterminism entirely, and are therefore handy when you can't be bothered
 with nondeterministic execution, though there are performance and flexibility
 costs to this.
\end_layout

\begin_layout Section
Goals
\end_layout

\begin_layout Standard
The goals of this project is to:
\end_layout

\begin_layout Enumerate
Give a desciption of the paradigms relevant to safe concurrency, where safe
 is defined as lacking race conditions and making deadlocks easier to handle
\end_layout

\begin_layout Enumerate
Present the leading edge of implementations and other things that should
 influence a modern concurrency abstraction framework
\end_layout

\begin_layout Enumerate
Give an overview over how threads, channels etc.
 work in modern implementations
\end_layout

\begin_layout Enumerate
Propose a mechanism for safe concurrency in C
\end_layout

\begin_layout Enumerate
Prototype the solution mentioned
\end_layout

\begin_layout Section
Problems, Complaints and Whining
\end_layout

\begin_layout Standard
Initially the idea was to make a lightweight threading library for C.
 Although this idea ended up sort-of-realized, a couple of limitations in
 C and POSIX presented themselves.
 For example, a POSIX thread's argument is the only way it can communicate
 and everything has to be crammed into this one struct.
 The result is a highly unpleasant nesting of structs, held together with
 pointers.
 Pointers, in turn, turn out to not be typechecked, and any significant
 refactoring resulted in unhelpful runtime errors.
 These problems, mutually reinforcing as they are, also reinforce the last
 problem: Mutexes as highest synchronization mechanism and the use of pointers
 to point to them.
 In the end, changing anything in the code became increasingly hard to reason
 about, making programming a nervous experience.
\end_layout

\begin_layout Standard
C11 has introduced a number of optional handy features, which would have
 made the code more portable, and generally better.
 Unfortunately, neither clang nor GCC have <stdatomic.h> and <thread.h> yet.
 The support for generics is also laclustre.
 C11 introduced a way to use C-macros to, at runtime, use the correct function
 for an arbitrary argument to a fictive function with generic argument.
 However, this requires the programmer to account for all the possible types
 people want to use in advance, and I assume a certain amount of work, at
 the very least, is involved if you want to have generic members of structs.
 Maybe using the lack of typechecking of pointers to structs would make
 this possible.
\end_layout

\begin_layout Standard
It also turned out to be hard to insulate the user from the complexities
 of the framework, though in retrospect an interface like that used by POSIX
 threads is probably possible.
 It's my fault, to be honest.
\end_layout

\begin_layout Standard
In short, working with a system language like C has its problems.
 For a funnier take, see 
\begin_inset CommandInset citation
LatexCommand cite
key "Mickens:NW"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Part
Background
\end_layout

\begin_layout Section
Organization
\end_layout

\begin_layout Standard
This part is going to be relatively unorganized.
 There's no neat progression to concurrency techniques.
 Some compete and some are used for different things.
 I don't expect one of the approaches here to become completely dominant,
 but rather that a mixture of approaches will be used.
 While I try to keep the introductory material at towards the front and
 the more intricate material towards the back but some arbitrariness is
 unavoidable, I think.
\end_layout

\begin_layout Section
Tl;DR
\end_layout

\begin_layout Itemize

\emph on
Semaphore
\emph default
: C, C++, Java etc.
 implement this style of concurrency.
 
\end_layout

\begin_deeper
\begin_layout Itemize
STM: Software Transactional Memory, or, for some architectures, simply Transacti
onal Memory, is a kind of optimistic semaphore technique where changes are
 rolled back and attempted again in case of conflict.
 Coupled with language features that prevent the sharing of non-transactional
 memory between threads it is a convenient, but computationally costly technique.
\end_layout

\end_deeper
\begin_layout Itemize

\emph on
Channel
\emph default
: This is the CSP family and its derivatives.
 Go, Rust, Haskell and Occam have concurrency mechanisms based on channels.
 Focus on being deterministic and formally analyzable.
 Uses multi-sender, multi-receiver (first to read) synchronous channels.
\end_layout

\begin_layout Itemize

\emph on
Actor
\emph default
: Erlang and Akka implement this model.
 Intuitively suited for making distributed systems, with a focus on fault
 tolerance rather than fault avoidance.
 Both implementations eschew sophisticated error recovery mechanisms and
 instead prefer crashing and rebooting errant processes, a philosophy called
 
\begin_inset Quotes eld
\end_inset

let it crash
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Itemize

\emph on
Paralell statements
\emph default
: If you have multiple actions that you know can be done in paralell without
 communicating with each other, then proceeding when all are done, then
 there's no reason you shouldn't.
 Rust and Akka offer this as futures, Occam has par blocks and LabView has
 paralell pipes.
 There's also parallel transformations of lists, which is particularly easy
 to do for the programmer.
 Often characterised as data-parallelism, as opposed to task-parallelism.
\end_layout

\begin_layout Itemize
Synchronous execution: Threads execute in synchrony, resulting in complete
 determinism.
 Esterel uses this.
\end_layout

\begin_layout Itemize
Lightweight threads that are less omputationally costly to deal with than
 OS threads, which in turn are less costly than processes.
 Details differ from implementation to implementation.
\end_layout

\begin_layout Section
Not Included: Cray's Chapel
\begin_inset Index idx
status open

\begin_layout Plain Layout
Chapel
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Most of what's covered in this report is pretty new, but at least somewhat
 mature.
 In order for programming languages to mature a lot of time is needed, and
 even the newest, not-out-of-beta-yet language here, Rust, is transitioning
 to being used in production right now.
 They also cover reasonably common scenarios: PC hardware, maybe with some
 distributed computing tossed in.
 Cray, a company making supercomputers, is cooperating with academics and
 other interested parties in making a new open source programming language
 for supercomputing: Chapel.
 Supercomputing has long been the domain of C/C++ and FORTRAN.
 Chapel is tasked with bringing supercomputer programming closer to the
 convenience of mainstream (Java, Python, Matlab etc.) while sacrificing
 no low-level control and providing as many parallel computation paradigms
 as possible.
 This could lead to exciting things in the future.
\end_layout

\begin_layout Section
Limitations of Lightweight Threads
\begin_inset Index idx
status open

\begin_layout Plain Layout
Limitations of Lightweight Threads
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Rust is the newest and hottest language investigated in this project.
 It's developed by Mozilla as a replacement for C++.
 Rust 
\emph on
used to have
\emph default
 only lightweight threads
\begin_inset CommandInset citation
LatexCommand cite
key "Rust heavyweight threads"

\end_inset

, but due to the many features desirable for threads in Rust (ability to
 call C code etc.) the lightweight threads ended up being as heavy to run
 as OS threads, and Rust's lightweight threads have been pushed to a library
 outside the standard library.
 This illustrates an important point: OS threads are not heavyweight just
 to irritate us, they are heavyweight because they have features lightweight
 threads don't.
 Tradeoffs are necessary.
 There is no free lunch.
 This is also true for the communication abstractions.
\end_layout

\begin_layout Standard
Let there be no mistake: In switching from OS threads with semaphores to
 any of the higher abstractions we sacrifice flexibility and usually performance
 for the sake of safety and ease of programming.
 That's the disclaimer.
 Now for the good part: The end result can still be more performant and
 do more complicated things because the safety and ease of use lets us program
 less conservatively.
 It's a bit like going from a low-level language to a high-level one.
\end_layout

\begin_layout Section
Lightweight Threads and Communication
\begin_inset Index idx
status open

\begin_layout Plain Layout
Lightweight Thread
\end_layout

\end_inset


\begin_inset Index idx
status open

\begin_layout Plain Layout
Comunication
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Lightweight threads and Communication mechanisms are different things not
 really intimately linked.
 It's quite possible to have one without the other.
 However, the motivation that lead to both is the same: In the multicore
 present, bug-free concurrent code must become easier to write.
 Since they share motivations, seeing them together is the norm.
 If your aim is to create a better way to write concurrent code, why only
 go half way, after all? And frankly, wanting lightweight threads so you
 can debug even more threads running with semaphores doesn't make sense
 to me.
 In fact that sounds like a mixture of masochism, code golf and performance
 art.
\end_layout

\begin_layout Standard
Lightweight threads have influenced the design of modern Communication mechanism
s.
 Since having lots of lightweight threads and switching between them is
 cheap, blocking calls also become cheap.
 Consequently, both the actor- and CSP-model can afford to have their threads
 mostly sitting around waiting for messages.
\end_layout

\begin_layout Section
The Nature of a Lightweight Thread
\begin_inset Index idx
status open

\begin_layout Plain Layout
Lightweight Thread
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Process > OS Thread > Lightweight Thread
\end_layout

\begin_layout Standard
OS threads are actually themselves lightweight when compared to another
 popular concurrency abstraction: processes.
 All processes run concurrently anyway, and moving information between them
 with inter-process communication (example: localhost) you suddenly have
 a heavyweight actor model: All that's old is new again, it seems.
 My point is that the quest for more efficient means of concurrency is hardly
 new, though it has acquired unusual urgency as single-core processor performanc
e gains are slowing down.
\end_layout

\begin_layout Paragraph
What does a lightweight thread look like? 
\end_layout

\begin_layout Standard
Well, the point of a lightweight thread is to be light - it can't have all
 the things an OS thread does.
 A POSIX thread holds the following things in memory
\begin_inset CommandInset citation
LatexCommand cite
key "llnl.gov pthreads"

\end_inset

:
\end_layout

\begin_layout Itemize
Stack pointer 
\end_layout

\begin_layout Itemize
Registers 
\end_layout

\begin_layout Itemize
Scheduling properties (such as policy or priority) 
\end_layout

\begin_layout Itemize
Set of pending and blocked signals 
\end_layout

\begin_layout Itemize
Thread specific data such as the current execution stack.
\end_layout

\begin_layout Standard
Collectively, these things are called a thread context.
 A context switch is when a processor core pauses, switches its registers,
 points its stack pointer and program counter at the right memory adresses
 etc.
 and starts executing again.
 So lightweight threads, in order to be lightweight, have to sustain themselves
 on less than a thread context and/or have faster context switches, which
 are two goals with overlapping means.
 There are many ways from here: 
\end_layout

\begin_layout Itemize
Go's goroutines have no signals, but they do have a stack.
 The stack starts small and, like a vector, can be expanded by allocating
 a bigger piece of memory somewhere else and deallocating the original stack
 space.
 By having a stack, goroutines are a bit like normal threads as they can
 be resumed after a pause.
 Go's goroutines are implemented like coroutines where inserting yield and
 resume is done by the compiler.
\end_layout

\begin_layout Itemize
Akka's actors have no stack.
 How do they work then? The Java (OS) thread runs the actor's receive function
 until completion and then the stack is thrown away as the Java thread moves
 on to another actor.
 Clever and simple, but it requires a different way of programming than
 we're used to.
 As a result of this, Akka offers 2.5 million actors per GB of heap memory.
 
\end_layout

\begin_layout Section
The Conflict Between Fault Tolerance and Verification/Debugging
\begin_inset Index idx
status open

\begin_layout Plain Layout
Fault Tolerance
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Formal verification is desireable in many safety-critical programs.
 Unfortunate side-effects of fault-tolerant systems, for example actor systems,
 is that they 
\end_layout

\begin_layout Enumerate
make the program's behaviour more complex 
\end_layout

\begin_layout Enumerate
hide faults from the developer at runtime.
\end_layout

\begin_layout Standard
Both are problems when we want to ensure correctness or debug the program
 at runtime.
 The first problem can be partly solved by using frameworks or languages
 where the added complexities can be assumed bug free, though you still
 need to superficially understand how they work.
 The second problem can be partly solved by using extensive logging.
 These techniques can make debugging such a complex system less hard than
 it would otherwise be, but the formal analysis of such a system can still
 be an unsolveable problem due to the amount of states the software can
 be in.
 Consider the queues of messages actors have.
 Their content, even their types, are hard to predict at compile time.
 The sequence messages arrive in is nondeterministic and, Heisenberg-like,
 is changed if you try to observe it.
 
\end_layout

\begin_layout Standard
It is, in sum, unlikely that an implementation made using actor- or other
 fault-tolerant techniques can be verified directly, however using these
 models can help create systems that will resist non-software faults.
 Also, these tools can usually be used to emulate verifiable systems and
 although that's not as good as using a tool with built-in verifiable abstractio
ns, it will usually be better than constructing your own using semaphores
 and heavy threads.
\end_layout

\begin_layout Section
Immutability 
\begin_inset Index idx
status open

\begin_layout Plain Layout
immutability
\end_layout

\end_inset


\end_layout

\begin_layout Standard
If an object or value is immutable it means it can't be changed.
 Depending on whom you ask, this might mean that only the object itself
 need be unchangeable, or it could mean that the object and all the objects
 it references are unchangeable.
 To avoid the ambiguity, terms like 
\begin_inset Quotes eld
\end_inset

deeply immutable
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

invariant
\begin_inset Quotes erd
\end_inset

 can be used for the latter.
 Objects or values that are deeply immutable can be shared between threads
 without fear of adverse effects, which make this property very attractive.
 Functional languages and languages with functional influences like Haskell,
 Erlang, Scala, Clojure and Rust place heavy emphasis on immutability.
\end_layout

\begin_layout Section
Coroutines: Single-thread Interleaved Execution
\begin_inset Index idx
status open

\begin_layout Plain Layout
Coroutines
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Coroutines are a superclass of subroutines, what we ordinarliy call functions,
 and are mostly written the same way.
 While a function is called and then returns, with a clear caller-callee
 relationship, coroutines can yield and receive execution as many times
 as desired during its lifetime.
 Functions, to contrast, 
\begin_inset Quotes eld
\end_inset

resume
\begin_inset Quotes erd
\end_inset

 at the beginning and yield at the end.
 Coroutines are so called because they do co-operative scheduling - yielding
 is entirely voluntary and predictable on the part of the coroutine.
 Coroutines were first done in Assembly by Melvin Conway in 1958, and it's
 easy to understand why - yielding in assembly can be done by simply jumping
 into another 
\begin_inset Quotes eld
\end_inset

coroutine
\begin_inset Quotes erd
\end_inset

.
 Actually, Assembly usually has no concept of functions, let alone coroutines,
 so you just jump to somewhere else in program memory.
\end_layout

\begin_layout Subsection
Disadvantages
\end_layout

\begin_layout Standard
Coroutines have co-operative scheduling, which is cheap compared to pre-emptive
 but that means we're reliant on the programmer for parallelism.
 Any coroutine can hog procesing capacity for as long as it likes.
 Also, real threads don't share stack, so state held in stack somehow needs
 to be synchronised across threads.
\end_layout

\begin_layout Subsection
Problems in C
\end_layout

\begin_layout Standard
I've decided to provide you with a basic example of the mechanisms used.
 Here we have two functions, co1 and co2, pass control between one another,
 saving each other's positions on the stack in global environment variables.
 The variables need to be global, because jumping to a function higher up
 on the stack will mess up any variable, even const pointers, held on the
 stack in that function.
 It's probably the same for all functions higher up on the stack, but I
 haven't tested.
 That gave me some weird bug behaviour that was pretty hard to figure out.
 It's worth pointing out that the behaviour of longjumping to a function
 that is not the caller of the current function is 
\emph on
undefined
\emph default
; this is abuse of longjmp, but it is not unprecedented and does work.
 An alternative would be using Boost.context from the Boost library for C++,
 if that can be made to work.
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "breaklines=true,extendedchars=true,frame=lrtb,language=C,numbers=left,showstringspaces=false"
inline false
status open

\begin_layout Plain Layout

#include <stdio.h>
\end_layout

\begin_layout Plain Layout

#include <setjmp.h>
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

static jmp_buf environment1;
\end_layout

\begin_layout Plain Layout

static jmp_buf environment2;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

__attribute__ ((noreturn)) static void co2()
\end_layout

\begin_layout Plain Layout

{
\end_layout

\begin_layout Plain Layout

  int VAR = 5;
\end_layout

\begin_layout Plain Layout

  printf("VAR = %d
\backslash
n", VAR);
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  printf("In function co2 place 1
\backslash
n");
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  int ret = setjmp(environment2);
\end_layout

\begin_layout Plain Layout

  if (ret == 0)
\end_layout

\begin_layout Plain Layout

  {
\end_layout

\begin_layout Plain Layout

    longjmp(environment1, 1);
\end_layout

\begin_layout Plain Layout

  }
\end_layout

\begin_layout Plain Layout

  printf("In function co2 place 2
\backslash
n");
\end_layout

\begin_layout Plain Layout

  ret = setjmp(environment2);
\end_layout

\begin_layout Plain Layout

  if (ret == 0)
\end_layout

\begin_layout Plain Layout

  {
\end_layout

\begin_layout Plain Layout

    longjmp(environment1, 1);
\end_layout

\begin_layout Plain Layout

  }
\end_layout

\begin_layout Plain Layout

  printf("In function co2 place 3
\backslash
n");
\end_layout

\begin_layout Plain Layout

  printf("VAR = %d
\backslash
n", VAR);
\end_layout

\begin_layout Plain Layout

  longjmp(environment1, 1);
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

static void co1()
\end_layout

\begin_layout Plain Layout

{
\end_layout

\begin_layout Plain Layout

  printf("In function co1 place 1
\backslash
n");
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  int ret = setjmp(environment1);
\end_layout

\begin_layout Plain Layout

  if (ret == 0)
\end_layout

\begin_layout Plain Layout

  {
\end_layout

\begin_layout Plain Layout

    co2();
\end_layout

\begin_layout Plain Layout

  }
\end_layout

\begin_layout Plain Layout

  printf("In function co1 place 2
\backslash
n");
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  ret = setjmp(environment1);
\end_layout

\begin_layout Plain Layout

  if (ret == 0)
\end_layout

\begin_layout Plain Layout

  {
\end_layout

\begin_layout Plain Layout

    longjmp(environment2, 1);
\end_layout

\begin_layout Plain Layout

  }
\end_layout

\begin_layout Plain Layout

  printf("In function co1 place 3
\backslash
n");
\end_layout

\begin_layout Plain Layout

  ret = setjmp(environment1);
\end_layout

\begin_layout Plain Layout

  if (ret == 0)
\end_layout

\begin_layout Plain Layout

  {
\end_layout

\begin_layout Plain Layout

    longjmp(environment2, 1);
\end_layout

\begin_layout Plain Layout

  }
\end_layout

\begin_layout Plain Layout

  return;
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

int main()
\end_layout

\begin_layout Plain Layout

{
\end_layout

\begin_layout Plain Layout

    printf("Hello World
\backslash
n");
\end_layout

\begin_layout Plain Layout

    co1();
\end_layout

\begin_layout Plain Layout

    return 0;
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The output is as follows:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "breaklines=true,frame=lrtb,numbers=left"
inline false
status open

\begin_layout Plain Layout

Hello World 
\end_layout

\begin_layout Plain Layout

In function co1 place 1 
\end_layout

\begin_layout Plain Layout

VAR = 5 
\end_layout

\begin_layout Plain Layout

In function co2 place 1 
\end_layout

\begin_layout Plain Layout

In function co1 place 2 
\end_layout

\begin_layout Plain Layout

In function co2 place 2 
\end_layout

\begin_layout Plain Layout

In function co1 place 3 
\end_layout

\begin_layout Plain Layout

In function co2 place 3 
\end_layout

\begin_layout Plain Layout

VAR = 0 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Interestingly, the integer variable VAR changes value without ever having
 been touched since creation in the source code.
 Not only can we not synchronise stack state between real threads, we can't
 even keep it within one real thread.
 As explained, coroutines can be done with jumps, but stack variables are
 not conserved.
 At least not without using costly system calls or complicated assembly
 voodoo, which most lightweight thread libraries for C seems to use.
 This means that the normal way of programming will have to be constrained
 a bit - all variables will have to be held on the heap or in global variables.
\end_layout

\begin_layout Subsection
How it may be used to create general-purpose lightweight threads
\end_layout

\begin_layout Standard
Coroutines as hereto discussed might make some code more readable (especially
 consumer-producer relationships), but it does not help us create lightweight
 threads of the kind we want.
 But what if the coroutine didn't yield to another coroutine but to a scheduler?
 Indeed, this is how goroutines work in Go, although Go hides so much of
 the manual scheduling work that its authors decided to exchange c for g
 in the name to avoid confusion.
 
\end_layout

\begin_layout Standard
A possible way to use coroutines to achieve lightweight threads would be
 to have each lightweight thread be a function pointer and a struct held
 on heap where state can be saved.
 The function yields each time it awaits a message and when it is done.
 When a function yields, the executing thread goes back to the scheduler
 and starts or resumes another function.
 Messages are received through a special value in the struct.
\end_layout

\begin_layout Section
Semaphores
\begin_inset Index idx
status open

\begin_layout Plain Layout
Semaphores
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Semaphores, mutexes and locks operate mostly the same.
 They are atomic variables that the threads can use to negotiate access
 to values they want to mutate.
 Unfortunately the negotiation has to be written by the programmer and this
 process is notoriously difficult.
 The solution, as with manual memory management, is to be very very careful.
 Concurrent threads usually do not persist during the entire program, but
 are created for specific purposes and ended as soon as possible to keep
 complexity as low as possible.
\end_layout

\begin_layout Standard
Simple example of the related mutex concept:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "breaklines=true,frame=lrtb,language=C,numbers=left,showstringspaces=false"
inline false
status open

\begin_layout Plain Layout

#include <stdio.h>
\end_layout

\begin_layout Plain Layout

#include <pthread.h>
\end_layout

\begin_layout Plain Layout

#include <assert.h>
\end_layout

\begin_layout Plain Layout

#include <stdlib.h>
\end_layout

\begin_layout Plain Layout

#include <time.h>
\end_layout

\begin_layout Plain Layout

#include <unistd.h>
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

#define ITERATIONS 1000000
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

pthread_mutex_t mutex;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

static void *increment(void *arg)
\end_layout

\begin_layout Plain Layout

{
\end_layout

\begin_layout Plain Layout

  long *counter = arg;
\end_layout

\begin_layout Plain Layout

  for(long i=0; i<ITERATIONS; i++)
\end_layout

\begin_layout Plain Layout

  {
\end_layout

\begin_layout Plain Layout

    pthread_mutex_lock(&mutex);
\end_layout

\begin_layout Plain Layout

    *counter = *counter+1;
\end_layout

\begin_layout Plain Layout

    pthread_mutex_unlock(&mutex);
\end_layout

\begin_layout Plain Layout

  }
\end_layout

\begin_layout Plain Layout

  return 0;
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

static void *decrement(void *arg)
\end_layout

\begin_layout Plain Layout

{
\end_layout

\begin_layout Plain Layout

  long *counter = arg;
\end_layout

\begin_layout Plain Layout

  for(long i=0; i<ITERATIONS; i++)
\end_layout

\begin_layout Plain Layout

  {
\end_layout

\begin_layout Plain Layout

    pthread_mutex_lock(&mutex);
\end_layout

\begin_layout Plain Layout

    *counter = *counter-1;
\end_layout

\begin_layout Plain Layout

    pthread_mutex_unlock(&mutex);
\end_layout

\begin_layout Plain Layout

  }
\end_layout

\begin_layout Plain Layout

  return 0;
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

int main(void)
\end_layout

\begin_layout Plain Layout

{
\end_layout

\begin_layout Plain Layout

  printf("Hello World
\backslash
n");
\end_layout

\begin_layout Plain Layout

  long *counter = malloc(sizeof(long));
\end_layout

\begin_layout Plain Layout

  *counter = 0;
\end_layout

\begin_layout Plain Layout

  printf("in main before threads counter=%ld
\backslash
n", *counter);
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  pthread_t *threads = malloc(2*sizeof(pthread_t));
\end_layout

\begin_layout Plain Layout

  pthread_mutex_t *mutex;
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  int rc;
\end_layout

\begin_layout Plain Layout

  rc = pthread_create(&threads[0], NULL, increment, counter);
\end_layout

\begin_layout Plain Layout

  assert(0 == rc);
\end_layout

\begin_layout Plain Layout

  rc = pthread_create(&threads[1], NULL, decrement, counter);
\end_layout

\begin_layout Plain Layout

  assert(0 == rc);
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  for (int i=0; i<2; ++i) 
\end_layout

\begin_layout Plain Layout

  {
\end_layout

\begin_layout Plain Layout

    // block until thread i completes
\end_layout

\begin_layout Plain Layout

    rc = pthread_join(threads[i], NULL);
\end_layout

\begin_layout Plain Layout

    printf("Thread number %d is complete
\backslash
n", i);
\end_layout

\begin_layout Plain Layout

    assert(0 == rc);
\end_layout

\begin_layout Plain Layout

  }
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  printf("in main after threads counter=%ld
\backslash
n", *counter);
\end_layout

\begin_layout Plain Layout

  return 0;
\end_layout

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
If you're anything like me you don't like this very much.
\end_layout

\begin_layout Section
Software Transactional Memory
\begin_inset Index idx
status open

\begin_layout Plain Layout
Software Transactional Memory
\end_layout

\end_inset


\begin_inset Index idx
status open

\begin_layout Plain Layout
STM
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Software Transactional Memory, STM for short, is a very convenient way to
 get communication between threads going.
 
\begin_inset Quotes eld
\end_inset

You there!
\begin_inset Quotes erd
\end_inset

, one can imagine the salesman go, 
\begin_inset Quotes eld
\end_inset

You there programming with semaphores! Wouldn't it be great if you could
 just make the semaphores disappear? With some STM you can!
\begin_inset Quotes erd
\end_inset

 Gosh, sounds great, doesn't it? So how does STM work? Well, STM is, as
 the name implies, transactional.
 Hang on:
\end_layout

\begin_layout Enumerate
You lock, copy (to C1) and unlock the variable you want to perform a calculation
 on (O).
\end_layout

\begin_layout Enumerate
You perform the calculation 
\end_layout

\begin_layout Enumerate
If the calculation involves writing to the original variable, then you write
 the result to a new copy (C2)
\end_layout

\begin_layout Enumerate
Now you lock the original variable (O), see if it is equal to C1.
 If O==C1, then the result from the calculation is still valid.
 If O!=C1 then you have to go back to step 1 (you never changed the original,
 so you simply delete C1 and C2).
\end_layout

\begin_layout Enumerate
If the calculation involved writing to the original variable (O), then you
 assign the second copy to the original (O=C2)
\end_layout

\begin_layout Enumerate
Now you have to unlock O again.
\end_layout

\begin_layout Standard
Of course it's done a bit more smartly than this in serious implementations,
 but it looks costly doesn't it? Unfortunately it is, but it's expected
 to improve a lot when we get hardware implementations.
 Well, actually we have hardware implementations, but not the sort that
 are targeted by mainstream compilers.
 A developer wishing to use Intel's (unfortunately buggy and off by default)
 TSX instructions will have to get dirty, probably with assembly, as these
 instructions were first available with the Haswell generation.
 If you're the sort whose processors come from Oracle or IBM, then your
 ecosystem might have had more time to nicely abstract these things away
 for you.
\end_layout

\begin_layout Standard
STM implementations do not have guarantees pleasing to the ear of a real-time
 engineer.
 Typically, operations will be done eventually, in arbitrary sequence.
 Unless something smart is done I don't see anything preventing starvation
 of a thread doing a particularly lengthy operation on an STM variable when
 others are doing short ones.
\end_layout

\begin_layout Standard
Whatever your langauge is, someone's probably made an STM implementation
 for it, but Clojure and Haskell are notable for their communities preferring
 them as their primary means of inter-thread communication.
 
\end_layout

\begin_layout Standard
Here's an example in Haskell (shamelessly taken from 
\begin_inset CommandInset citation
LatexCommand cite
key "Haskell STM Example"

\end_inset

):
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "breaklines=true,frame=lrtb,language=Haskell,numbers=left,showstringspaces=false"
inline false
status open

\begin_layout Plain Layout

module Main where
\end_layout

\begin_layout Plain Layout

import Control.Monad
\end_layout

\begin_layout Plain Layout

import Control.Concurrent
\end_layout

\begin_layout Plain Layout

import Control.Concurrent.STM
\end_layout

\begin_layout Plain Layout

 
\end_layout

\begin_layout Plain Layout

main = do shared <- atomically $ newTVar 0
\end_layout

\begin_layout Plain Layout

          before <- atomRead shared
\end_layout

\begin_layout Plain Layout

          putStrLn $ "Before: " ++ show before
\end_layout

\begin_layout Plain Layout

          forkIO $ 25 `timesDo` (dispVar shared >> milliSleep 20)
\end_layout

\begin_layout Plain Layout

          forkIO $ 10 `timesDo` (appV ((+) 2) shared >> milliSleep 50)
\end_layout

\begin_layout Plain Layout

          forkIO $ 20 `timesDo` (appV pred shared >> milliSleep 25)
\end_layout

\begin_layout Plain Layout

          milliSleep 800
\end_layout

\begin_layout Plain Layout

          after <- atomRead shared
\end_layout

\begin_layout Plain Layout

          putStrLn $ "After: " ++ show after
\end_layout

\begin_layout Plain Layout

 where timesDo = replicateM_
\end_layout

\begin_layout Plain Layout

       milliSleep = threadDelay .
 (*) 1000
\end_layout

\begin_layout Plain Layout

 
\end_layout

\begin_layout Plain Layout

atomRead = atomically .
 readTVar
\end_layout

\begin_layout Plain Layout

dispVar x = atomRead x >>= print
\end_layout

\begin_layout Plain Layout

appV fn x = atomically $ readTVar x >>= writeTVar x .
 fn
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This program runs for 800 milliseconds and has three threads.
 A transactional integer is made.
 Thread A prints the integer every 20 milliseconds, thread B adds two to
 the integer every 50 milliseconds and thread C subtracts one from the integer
 every 25 milliseconds.
 After 800 milliseconds the integer is 0 again.
 At least that's what I think it does.
 
\end_layout

\begin_layout Standard
Here's a Clojure example illustrating STM, and this one I've actually written
 myself:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "frame=lrtb,numbers=left"
inline false
status open

\begin_layout Plain Layout

(def x (ref 1))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

(defn increment [i]
\end_layout

\begin_layout Plain Layout

  (if (> i 0) 
\end_layout

\begin_layout Plain Layout

    (
\end_layout

\begin_layout Plain Layout

      (dosync
\end_layout

\begin_layout Plain Layout

        (alter x inc)
\end_layout

\begin_layout Plain Layout

      )
\end_layout

\begin_layout Plain Layout

      (Thread/sleep 1)
\end_layout

\begin_layout Plain Layout

      (increment (- i 1))
\end_layout

\begin_layout Plain Layout

    )
\end_layout

\begin_layout Plain Layout

  )
\end_layout

\begin_layout Plain Layout

)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

(defn decrement [i]
\end_layout

\begin_layout Plain Layout

  (if (> i 0)
\end_layout

\begin_layout Plain Layout

    (
\end_layout

\begin_layout Plain Layout

      (dosync
\end_layout

\begin_layout Plain Layout

        (alter x dec)
\end_layout

\begin_layout Plain Layout

      )
\end_layout

\begin_layout Plain Layout

      (Thread/sleep 1)
\end_layout

\begin_layout Plain Layout

      (decrement (- i 1))
\end_layout

\begin_layout Plain Layout

    )
\end_layout

\begin_layout Plain Layout

  )
\end_layout

\begin_layout Plain Layout

)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

(defn printref [i]
\end_layout

\begin_layout Plain Layout

  (if (> i 0) 
\end_layout

\begin_layout Plain Layout

    (
\end_layout

\begin_layout Plain Layout

      (dosync
\end_layout

\begin_layout Plain Layout

        (println (format "in printref %d" @x))
\end_layout

\begin_layout Plain Layout

      )
\end_layout

\begin_layout Plain Layout

      (Thread/sleep 1)
\end_layout

\begin_layout Plain Layout

      (printref (- i 1))
\end_layout

\begin_layout Plain Layout

    )
\end_layout

\begin_layout Plain Layout

  )
\end_layout

\begin_layout Plain Layout

)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

(future
\end_layout

\begin_layout Plain Layout

  (increment 10)
\end_layout

\begin_layout Plain Layout

)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

(future
\end_layout

\begin_layout Plain Layout

  (printref 15)
\end_layout

\begin_layout Plain Layout

)
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

(future
\end_layout

\begin_layout Plain Layout

  (decrement 10)
\end_layout

\begin_layout Plain Layout

)
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Handling large STM variables
\end_layout

\begin_layout Standard
My explanation of STM above has a problem in it: What if the STM variable
 is really really large? In the real world, STM algorithms do not copy,
 but use logs of the specific thing they read and write: A read-set and
 a write-set.
 Copies or records will necessarily have to be made, but if you only read
 and write to element 23438 in the array then whether the other variables
 have been changed during operation doesn't matter.
 Not only does this improve memory usage, but other threads can work concurrentl
y on other parts of the array without much conflict.
\end_layout

\begin_layout Section
Channels
\begin_inset Index idx
status open

\begin_layout Plain Layout
Channels
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A channel is a communication primitive between threads.
 A channel, as used in Go, Rust and CSP, can have several senders and recipients.
 Each message can typically only be received once, so multiple receivers
 is not a broadcast mechanism, but rather a work division mechanism.
\end_layout

\begin_layout Standard
Channels as a method for communication are analogous to the concurrency
 in CSP, and convenient to verify and do formal analysis upon.
 The most known languages impementing them are Rust and Go.
\end_layout

\begin_layout Standard
Rust uses channels, futures and immutable copies.
 Channels are multi-sender, multi-receiver affairs, CSP-style:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "breaklines=true,frame=lrtb,language={C++},numbers=left,showstringspaces=false"
inline false
status open

\begin_layout Plain Layout

fn main() 
\end_layout

\begin_layout Plain Layout

{   
\end_layout

\begin_layout Plain Layout

	let (tx, rx) = channel();   
\end_layout

\begin_layout Plain Layout

	let txclone = tx.clone();
\end_layout

\begin_layout Plain Layout

	
\end_layout

\begin_layout Plain Layout

	//proc denotes an anonymous function with function body being the stuff
 behind "proc()"
\end_layout

\begin_layout Plain Layout

	spawn(proc() tx.send("message"));
\end_layout

\begin_layout Plain Layout

	spawn(proc() txclone.send("another message"));      
\end_layout

\begin_layout Plain Layout

	spawn(proc() println!("{:s}, {:s}" ,rx.recv(), rx.recv()) ); 
\end_layout

\begin_layout Plain Layout

} 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
When sending a message over a channel in Rust, the data in the message will
 be duplicated.
 To improve efficiency, multiple tasks can share an object as if on a shared
 heap, provided that the object is immutable.
 You might have noticed that I cloned the transmitter.
 Why? Rust has no garbage collection, but still automatic memory management,
 which means that the moment at which an object can be deallocated from
 the heap need to be apparent at compile time.
 Therefore, two threads can't have references to the same heap object, in
 this case the transmitter.
 In Rust parlance, a thread owns the object and two threads can't own the
 same object.
\end_layout

\begin_layout Standard
Go takes a more imperative approach, and accidental data sharing is easier
 to do.
 Here we're doing the exact same thing in Go.
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "breaklines=true,frame=lrtb,numbers=left,showstringspaces=false"
inline false
status open

\begin_layout Plain Layout

package main
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

import "fmt"
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

func main() {
\end_layout

\begin_layout Plain Layout

  messages := make(chan string)
\end_layout

\begin_layout Plain Layout

  go func() { messages <- "message" }()
\end_layout

\begin_layout Plain Layout

  go func() { messages <- "another message" }()
\end_layout

\begin_layout Plain Layout

  msg := <-messages
\end_layout

\begin_layout Plain Layout

  msg2 := <-messages
\end_layout

\begin_layout Plain Layout

  fmt.Println(msg + ", " + msg2)
\end_layout

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Since Go has garbage collection, there's no need for cloning.
\end_layout

\begin_layout Section
Actors
\begin_inset Index idx
status open

\begin_layout Plain Layout
Actors
\end_layout

\end_inset


\end_layout

\begin_layout Standard
There are many implementations of the Actor model, but only two that I know
 of that matter: Erlang and Akka.
 
\end_layout

\begin_layout Subsection
Akka on Scala
\begin_inset Index idx
status open

\begin_layout Plain Layout
Akka
\end_layout

\end_inset


\begin_inset Index idx
status open

\begin_layout Plain Layout
Scala
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Scala, a language, originally had its own message passing library, but this
 has been deprecated in favor of the Akka library, the essentials of which
 is included as standard in all recent distributions of Scala.
 The Akka actor library is an improvement on the Scala actor library which
 in turn is based on the Erlang actor model.
 Akka is also available to Java programmers.
\end_layout

\begin_layout Standard
It's used like this:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "breaklines=true,frame=lrtb,language=scala,numbers=left"
inline false
status open

\begin_layout Plain Layout

import akka.actor.Actor 
\end_layout

\begin_layout Plain Layout

import akka.actor.ActorRef 
\end_layout

\begin_layout Plain Layout

import akka.actor.ActorSystem 
\end_layout

\begin_layout Plain Layout

import akka.actor.Props 
\end_layout

\begin_layout Plain Layout

import scala.sys 
\end_layout

\begin_layout Plain Layout

import java.lang.Thread
\end_layout

\begin_layout Plain Layout

//Type declarations look like this: "nameOfObject:NameOfClass" with or without
 space.
\end_layout

\begin_layout Plain Layout

class DemoActor(printstr:String) extends Actor 
\end_layout

\begin_layout Plain Layout

{  
\end_layout

\begin_layout Plain Layout

  def receive = 
\end_layout

\begin_layout Plain Layout

  {     
\end_layout

\begin_layout Plain Layout

    case other: ActorRef =>
\end_layout

\begin_layout Plain Layout

    {     	
\end_layout

\begin_layout Plain Layout

      println(printstr)     	
\end_layout

\begin_layout Plain Layout

      other ! self      
\end_layout

\begin_layout Plain Layout

    }     
\end_layout

\begin_layout Plain Layout

    case _       => println("unknown message")   
\end_layout

\begin_layout Plain Layout

  } 
\end_layout

\begin_layout Plain Layout

}   
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

object Main extends App 
\end_layout

\begin_layout Plain Layout

{   
\end_layout

\begin_layout Plain Layout

  val system = ActorSystem("DemoSystem")     
\end_layout

\begin_layout Plain Layout

  val aActor = system.actorOf(Props(classOf[DemoActor], "a"))   
\end_layout

\begin_layout Plain Layout

  val bActor = system.actorOf(Props(classOf[DemoActor], "b"))   
\end_layout

\begin_layout Plain Layout

  aActor ! bActor   
\end_layout

\begin_layout Plain Layout

  Thread.sleep(3)   //let actors run for 3 milliseconds
\end_layout

\begin_layout Plain Layout

  scala.sys.exit() 
\end_layout

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The output of this is a sequence of type 
\begin_inset Quotes eld
\end_inset

a b a b a b
\begin_inset Quotes erd
\end_inset

 with each letter on a line of its own.
 The precise number of a's and b's printed to the console varies, but in
 general you don't have to scroll if your console is maximised.
 In another example, creating a million Akka actors in Scala takes about
 11 to 12 seconds on the lab computers.
\end_layout

\begin_layout Standard
Synchronous messaging is neither enforced nor recommended by the Akka developers
, but you can use bounded and blocking mailboxes.
 A bounded and blocking mailbox will block the sender of a message if the
 receiver's message queue is full but the minimum capacity is 1, so true
 synchrony is not achieved.
 To achieve true synchrony you need to manually use Await for each time
 you send a message.
 The reason it is not recommended is that new classes of bugs surface, something
 that might be advantageous to a life-critical application programmer, but
 not to the vast majority of programmers.
\end_layout

\begin_layout Standard
Akka actors interface neatly with other concurrency abstractions in Akka.
 More in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Actors,-Futures-and"

\end_inset

.
\end_layout

\begin_layout Standard
Akka actors have many interesting properties.
 For example, actor references are network aware
\begin_inset Index idx
status open

\begin_layout Plain Layout
Network awareness
\end_layout

\end_inset

, so one of our DemoActors could receive a message with an ActorRef to an
 actor on another continent and it would still respond correctly using the
 Akka Remote Protocol over TCP.
 Akka, unlike Erlang, enforces supervision in a similar manner as offered
 by Erlang's OTP.
\end_layout

\begin_layout Standard
Akka actor reference examples
\begin_inset CommandInset citation
LatexCommand cite
key "Akka actorRefs"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "frame=lrtb"
inline false
status open

\begin_layout Plain Layout

"akka://my-sys/user/service-a/worker1"                   // purely local
 
\end_layout

\begin_layout Plain Layout

"akka.tcp://my-sys@host.example.com:5678/user/service-b" // remote
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Akka offers configurable message dispatchers, offering control over the
 number of underlying threads and number of messages an actor can process
 before the underlying thread jumps to the next actor.
 The default dispatcher uses a fork-join method to run the actors, but you
 can also use thread-pool or your own custom dispatcher.
\end_layout

\begin_layout Standard
Akka actors send messages as references by default, and unfortunately neither
 Java nor Scala enforces immutability of the underlying objects.
 To work around this, Akka provides optional deep copying of all messages.
\end_layout

\begin_layout Subsection
Erlang
\begin_inset Index idx
status open

\begin_layout Plain Layout
Erlang
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Both Erlang and OTP were developed at Ericsson, and today are used for critical
 infrastructure worth multiples of billions, like WhatsApp.
 Insert snarky remark about capitalism here.
 OTP stands for Open Telephony Platform,
\begin_inset Index idx
status open

\begin_layout Plain Layout
OTP
\end_layout

\end_inset

 an archaic name not reflecting current use.
\end_layout

\begin_layout Standard
For Erlang, the actor model is the beginning and the end.
 Erlang is structured around them; each actor has its own heap, which means
 that everything sent is physically copied.
 Erlang is also an eraly functional programming language, and has no loops.
 State is held as recursion arguments as the actors main function calls
 itself at the end of processing a message (or don't, if you want to terminate
 the actor).
\end_layout

\begin_layout Standard
Erlang can use something called selective receives to prioritize certain
 kinds of messages.
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "breaklines=true,frame=lrtb,language=erlang,numbers=left,showstringspaces=false"
inline false
status open

\begin_layout Plain Layout

-module(kitchen).
 
\end_layout

\begin_layout Plain Layout

-compile(export_all).
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

fridge2(FoodList) ->     
\end_layout

\begin_layout Plain Layout

	receive         
\end_layout

\begin_layout Plain Layout

		{From, {store, Food}} ->             
\end_layout

\begin_layout Plain Layout

			From ! {self(), ok},             
\end_layout

\begin_layout Plain Layout

			fridge2([Food|FoodList]);         
\end_layout

\begin_layout Plain Layout

		{From, {take, Food}} ->             
\end_layout

\begin_layout Plain Layout

			case lists:member(Food, FoodList) of                 
\end_layout

\begin_layout Plain Layout

				true ->                     
\end_layout

\begin_layout Plain Layout

					From ! {self(), {ok, Food}},                     
\end_layout

\begin_layout Plain Layout

					fridge2(lists:delete(Food, FoodList));                 
\end_layout

\begin_layout Plain Layout

				false ->                     
\end_layout

\begin_layout Plain Layout

					From ! {self(), not_found},                     
\end_layout

\begin_layout Plain Layout

					fridge2(FoodList)             
\end_layout

\begin_layout Plain Layout

				end;         
\end_layout

\begin_layout Plain Layout

		terminate ->             
\end_layout

\begin_layout Plain Layout

			ok     
\end_layout

\begin_layout Plain Layout

	end.
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

store(Pid, Food) ->     
\end_layout

\begin_layout Plain Layout

	Pid ! {self(), {store, Food}},     
\end_layout

\begin_layout Plain Layout

	receive         
\end_layout

\begin_layout Plain Layout

		{Pid, Msg} -> Msg     
\end_layout

\begin_layout Plain Layout

	end.
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

take(Pid, Food) ->     
\end_layout

\begin_layout Plain Layout

	Pid ! {self(), {take, Food}},     
\end_layout

\begin_layout Plain Layout

	receive         
\end_layout

\begin_layout Plain Layout

		{Pid, Msg} -> Msg     
\end_layout

\begin_layout Plain Layout

	end.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Synchronous Execution
\end_layout

\begin_layout Standard
I'v'e only found one language that does this, and that is:
\end_layout

\begin_layout Subsection
Esterel
\begin_inset Index idx
status open

\begin_layout Plain Layout
Esterel
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Esterel may be the coolest implementation discussed here.
 It has very limited expressive power and is not freely available, but it
 has 
\emph on
exciting
\emph default
 properties: Threads in Esterel 
\emph on
execute in synchronous time
\emph default
.
 Put differently, Esterel has a global clock that threads march in lockstep
 to! A thread has a loop and one cycle of that loop is ompleted per tick
 of the global clock.
 Esterel is completely deterministic; neither dynamic memory nor spawning
 of processes are supported.
 Signals are broadcast, and threads await and send signals.
 A signal is either precent in a cycle or it is not - the time of broadcast
 is abstracted away.
 Programs in Esterel are deterministic finite state machines.
\end_layout

\begin_layout Standard
Esterel is not a real-time language but since it is completely deterministic,
 simple testing should suffice for investigating time characteristics.
\end_layout

\begin_layout Standard
Esterel is hard to get hold of as a private individual, is a niche language
 and therefore we won't explore it in more detail.
\end_layout

\begin_layout Section
Pipelining
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pipeline.svg
	lyxscale 5
	width 40text%

\end_inset


\end_layout

\begin_layout Standard
If you have a constantly incoming stream of data coming in to the leftmost
 block in the picture, you have an opportunity to pipeline.
 This concept should be familiar to you if you've ever studied processor
 design, where pipelining means you piece up computations into small parts
 that can be done after each other.
 If you've got a lot of similar computations coming in after each other
 then you can do each step in parallel.
 Think of how an assembly line doesn't necessarily make any given car faster
 than manual assembly, but since each step is done in parallel you get figures
 like 
\begin_inset Quotes eld
\end_inset

3 cars per minute
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
That's the concept here as well.
 If you place each step on its own processor core, you have an assembly
 line for computation.
 LabView uses this
\begin_inset CommandInset citation
LatexCommand cite
key "LabView parallelism"

\end_inset

.
\end_layout

\begin_layout Section
Parallel Statements
\end_layout

\begin_layout Standard
Parallel statements are very attractive, because they can make our code
 parallel without introducing indeterminism.
 The catch is that the statements to be parallelised cannot communicate
 with each other.
 Keep in mind, though, that the overhead of managing threads might outweigh
 the benefits of parallelism if the tasks you want done are sufficiently
 simple and few.
 Some tools and languages, like Occam, treat your parallel instructions
 as guidelines where this tradeoff is outsourced to the compiler.
 Others, like Scala, does exactly what you tell them, and there you should
 not use parallel statements unless your workload is heavy enough.
\end_layout

\begin_layout Subsection
Occam's PAR (without messages)
\begin_inset Index idx
status open

\begin_layout Plain Layout
Occam
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Unfortunately it proved troublesome to get one of the Occam compilers going
 - working with dead languages can be frustrating.
 Fortunately, many of the facilities offered in Occam can be replicated
 in other languages, for example by using Futures and Actors.
\end_layout

\begin_layout Standard
For example this (admittedly completely paper-programmed) Occam snippet
\begin_inset CommandInset label
LatexCommand label
name "Occam PAR"

\end_inset

...
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "frame=lrtb,language=ML,numbers=left,showstringspaces=false"
inline false
status open

\begin_layout Plain Layout

#INCLUDE "hostio.inc" 
\end_layout

\begin_layout Plain Layout

#USE "hostio.lib" 
\end_layout

\begin_layout Plain Layout

PROC Main(CHAN OF SP fs,ts)
\end_layout

\begin_layout Plain Layout

	SEQ
\end_layout

\begin_layout Plain Layout

		PAR
\end_layout

\begin_layout Plain Layout

			x = function(1)
\end_layout

\begin_layout Plain Layout

			y = function(2)
\end_layout

\begin_layout Plain Layout

		z = x+y
\end_layout

\begin_layout Plain Layout

		so.write.string.int(fs, ts, z, 0)
\end_layout

\begin_layout Plain Layout

		so.exit(fs,ts,sps.success)
\end_layout

\begin_layout Plain Layout

:
\end_layout

\end_inset


\end_layout

\begin_layout Standard
...has x and y be computed in parallel and then combined into z.
 The same thingcan be done with futures, and in 
\begin_inset CommandInset ref
LatexCommand ref
reference "Scala future example"

\end_inset

 we have a corresponding example in Scala.
 Of course a function inside a PAR can send messages to another inside the
 same PAR, while it is not unthinkable that other parallel statement constructs
 in other languages can do the same, it is usually not the intended use.
\end_layout

\begin_layout Subsection
LabView's parallel arrows
\begin_inset Index idx
status open

\begin_layout Plain Layout
LabView
\end_layout

\end_inset


\end_layout

\begin_layout Standard
LabView is a proprietary graphical programming language, and as such a bit
 different from the other languages discussed here.
 LabView uses parallel statements.
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename /home/tormod/Documents/skole/fall2014/uniproject/doc/VisParProg.png
	lyxscale 70
	width 40text%

\end_inset


\end_layout

\begin_layout Standard
I had problems getting hold of labview screnshots with an appropriate license.
 Anyway, the basic concept can be easily eplained even with limited fidelity.
 Data flows along the arrows and the blocks hold functions that outputs
 new data.
 As the dataflows branch, you can see the potential for paralellism.
 Block a and block b are completely independent and LabView can (and will)
\begin_inset CommandInset citation
LatexCommand cite
key "LabView parallelism"

\end_inset

 therefore execute them in paralell.
 In the end it all gets fed into a sink, for example a controller or a print
 to memory or screen.
 
\end_layout

\begin_layout Standard
An Occam programmer may express the sequence pictured above like this:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "frame=lrtb,numbers=left,showstringspaces=false"
inline false
status open

\begin_layout Plain Layout

SEQ
\end_layout

\begin_layout Plain Layout

	do stuff
\end_layout

\begin_layout Plain Layout

	PAR
\end_layout

\begin_layout Plain Layout

		a
\end_layout

\begin_layout Plain Layout

		b
\end_layout

\begin_layout Plain Layout

	do more stuff
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Futures
\end_layout

\begin_layout Standard
A future is an [insert value here]-value.
 It represents the result of a computation that may or may not have finished
 yet.
 It's a feature for requesting a computation and getting the results (including
 side effects) later.
 Here's a Rust example of the use of a Future
\begin_inset CommandInset label
LatexCommand label
name "Future example2"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "breaklines=true,frame=lrtb,language={C++},numbers=left,showstringspaces=false"
inline false
status open

\begin_layout Plain Layout

use std::sync::Future;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

fn main()  
\end_layout

\begin_layout Plain Layout

{      
\end_layout

\begin_layout Plain Layout

	fn fib(n: u64) -> u64    
\end_layout

\begin_layout Plain Layout

	{     
\end_layout

\begin_layout Plain Layout

		// lengthy computation returning an uint     
\end_layout

\begin_layout Plain Layout

		12586269025   
\end_layout

\begin_layout Plain Layout

	}
\end_layout

\begin_layout Plain Layout

	let mut delayed_fib = Future::spawn(proc() fib(50)); 
\end_layout

\begin_layout Plain Layout

	println!("fib(50) = {}", delayed_fib.get());
\end_layout

\begin_layout Plain Layout

} 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This Scala example 
\begin_inset CommandInset label
LatexCommand label
name "Scala future example"

\end_inset

 does the same as the Occam one in 
\begin_inset CommandInset ref
LatexCommand ref
reference "Occam PAR"

\end_inset

, just with futures:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "frame=lrtb,language=scala,numbers=left"
inline false
status open

\begin_layout Plain Layout

import scala.concurrent._ 
\end_layout

\begin_layout Plain Layout

import scala.concurrent.ExecutionContext.Implicits.global 
\end_layout

\begin_layout Plain Layout

import scala.concurrent.duration._ 
\end_layout

\begin_layout Plain Layout

import scala.language.postfixOps
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

object occ extends App 
\end_layout

\begin_layout Plain Layout

{   
\end_layout

\begin_layout Plain Layout

	def function(i: Int) = i*i
\end_layout

\begin_layout Plain Layout

	val x = Future {function(1)}   
\end_layout

\begin_layout Plain Layout

	val y = Future {function(2)}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	val z = for   
\end_layout

\begin_layout Plain Layout

	{     
\end_layout

\begin_layout Plain Layout

		xc <- x     
\end_layout

\begin_layout Plain Layout

		yc <- y   
\end_layout

\begin_layout Plain Layout

	} yield xc + yc   
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	val zval = Await.result(z, 0 nanos)   
\end_layout

\begin_layout Plain Layout

	println(zval) 
\end_layout

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Parallel List Transformation
\end_layout

\begin_layout Standard
Many languages support doing calculations on elements in a list in parallel.
 This is a very simple way to achieve parallelism; no undeterminism or other
 concerns.
 Here's how it's done in Scala: 
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "breaklines=true,frame=lrtb,language=scala,numbers=left"
inline false
status open

\begin_layout Plain Layout

val a = List(1,2,3,4,5,6,7,8,9)
\end_layout

\begin_layout Plain Layout

val b = a.par.map(x=>2*x).toList
\end_layout

\begin_layout Plain Layout

//b is List(2, 4, 6, 8, 10, 12, 14, 16, 18)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
While Haskell does the same like this:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "breaklines=true,frame=lrtb,language=Haskell,numbers=left,showstringspaces=false"
inline false
status open

\begin_layout Plain Layout

import Control.Parallel.Strategies
\end_layout

\begin_layout Plain Layout

let a = [1,2,3,4,5,6,7,8,9]
\end_layout

\begin_layout Plain Layout

let b = parMap rpar (*2) a
\end_layout

\begin_layout Plain Layout

//b is [2,4,6,8,10,12,14,16,18]
\end_layout

\end_inset


\end_layout

\begin_layout Standard
It is noteworthy that rpar makes this evaluation lazy, a sort of future
 if you will, in that the computation returns immediately and sequential
 computing afterwards will only wait when b is needed.
 Neat.
\end_layout

\begin_layout Standard
This type of concurrency is actually not too hard to achieve in C.
 Yours truly have made an admittedly array[int]->array[int] attempt
\begin_inset CommandInset label
LatexCommand label
name "C parmap"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "breaklines=true,frame=lrtb,language=C,numbers=left,showstringspaces=false"
inline false
status open

\begin_layout Plain Layout

#include <stdio.h>
\end_layout

\begin_layout Plain Layout

#include <unistd.h>
\end_layout

\begin_layout Plain Layout

#include <pthread.h>
\end_layout

\begin_layout Plain Layout

#include <stdlib.h>
\end_layout

\begin_layout Plain Layout

#include <assert.h>
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

struct ThreadArgument
\end_layout

\begin_layout Plain Layout

{
\end_layout

\begin_layout Plain Layout

  int numIterations;
\end_layout

\begin_layout Plain Layout

  int *iterationsStart;
\end_layout

\begin_layout Plain Layout

  int (*function)(int);
\end_layout

\begin_layout Plain Layout

};
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

void *singlethreaditerator(void *arg) //struct ThreadArgument* argument
\end_layout

\begin_layout Plain Layout

{ 
\end_layout

\begin_layout Plain Layout

  printf("Entered singlethreaditerator
\backslash
n");
\end_layout

\begin_layout Plain Layout

  struct ThreadArgument argument = *(struct ThreadArgument*)arg;
\end_layout

\begin_layout Plain Layout

  for(int i=0; i<argument.numIterations; i++)
\end_layout

\begin_layout Plain Layout

  {
\end_layout

\begin_layout Plain Layout

    *(argument.iterationsStart+i) = argument.function(*(argument.iterationsStart+i)
);
\end_layout

\begin_layout Plain Layout

  }
\end_layout

\begin_layout Plain Layout

  return NULL;
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

void intparmap(int array[], int arrayLength, int (*function)(int) )
\end_layout

\begin_layout Plain Layout

{
\end_layout

\begin_layout Plain Layout

  int numofcpus = sysconf(_SC_NPROCESSORS_ONLN);
\end_layout

\begin_layout Plain Layout

  int numofthreads = numofcpus;
\end_layout

\begin_layout Plain Layout

  pthread_t threads[numofthreads];
\end_layout

\begin_layout Plain Layout

  int thread_args[numofthreads];
\end_layout

\begin_layout Plain Layout

  int tasksPerThread = arrayLength/numofthreads;
\end_layout

\begin_layout Plain Layout

  printf("tasksPerThread = %d
\backslash
n", tasksPerThread);
\end_layout

\begin_layout Plain Layout

  struct ThreadArgument threadsarguments[numofthreads];
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

  for (int i=0; i<(numofthreads-1); i++) //handle first n-1 threads
\end_layout

\begin_layout Plain Layout

  {
\end_layout

\begin_layout Plain Layout

    printf("intparmap
\backslash
n");
\end_layout

\begin_layout Plain Layout

    threadsarguments[i].iterationsStart = &array[i*tasksPerThread];
\end_layout

\begin_layout Plain Layout

    threadsarguments[i].numIterations = tasksPerThread;
\end_layout

\begin_layout Plain Layout

    printf("intparmap
\backslash
n");
\end_layout

\begin_layout Plain Layout

    threadsarguments[i].function = function; 
\end_layout

\begin_layout Plain Layout

  }
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

  threadsarguments[numofthreads-1].iterationsStart = &array[(numofthreads-1)*task
sPerThread];
\end_layout

\begin_layout Plain Layout

  threadsarguments[numofthreads-1].numIterations = arrayLength-(numofthreads-1)*t
asksPerThread;
\end_layout

\begin_layout Plain Layout

  threadsarguments[numofthreads-1].function = function; 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

  int rc;
\end_layout

\begin_layout Plain Layout

  for (int i=0; i<(numofthreads); i++) //start threads
\end_layout

\begin_layout Plain Layout

  {
\end_layout

\begin_layout Plain Layout

    printf("Creating thread %d
\backslash
n", i);
\end_layout

\begin_layout Plain Layout

    rc = pthread_create(&threads[i], NULL, singlethreaditerator, (void *)
 &threadsarguments[i]);
\end_layout

\begin_layout Plain Layout

    assert(0 == rc);
\end_layout

\begin_layout Plain Layout

  }
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

  //wait for threads
\end_layout

\begin_layout Plain Layout

  for (int i=0; i<numofthreads; ++i) 
\end_layout

\begin_layout Plain Layout

  {
\end_layout

\begin_layout Plain Layout

    // block until thread i completes
\end_layout

\begin_layout Plain Layout

    rc = pthread_join(threads[i], NULL);
\end_layout

\begin_layout Plain Layout

    printf("Thread %d is complete
\backslash
n", i);
\end_layout

\begin_layout Plain Layout

    assert(0 == rc);
\end_layout

\begin_layout Plain Layout

  }
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

int square(int x)
\end_layout

\begin_layout Plain Layout

{
\end_layout

\begin_layout Plain Layout

  return x*x;
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

int main()
\end_layout

\begin_layout Plain Layout

{
\end_layout

\begin_layout Plain Layout

  printf("Hello World
\backslash
n");
\end_layout

\begin_layout Plain Layout

  int num = 1000;
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

  int array[num];
\end_layout

\begin_layout Plain Layout

  for(int i=0; i<num; i++)
\end_layout

\begin_layout Plain Layout

  {
\end_layout

\begin_layout Plain Layout

    array[i] = i;
\end_layout

\begin_layout Plain Layout

  }
\end_layout

\begin_layout Plain Layout

  intparmap(array, num, square);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

  for(int i=0; i<num; i++)
\end_layout

\begin_layout Plain Layout

  {
\end_layout

\begin_layout Plain Layout

    printf("%d
\backslash
n" , array[i]);
\end_layout

\begin_layout Plain Layout

  }
\end_layout

\begin_layout Plain Layout

  
\end_layout

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This might not be the best way to do it, but it is fairly transparent to
 the end user.
 The code here transforms any array of integers by changing each element
 with any int->int function.
 Notice that since there's no need to share information among threads, there's
 no need for mutexes, semaphores or other such mechanisms.
\end_layout

\begin_layout Standard
Scala's parallel statements are actually classes with parallel methods,
 so-called parallel collections.
 There are many ways these can be used and you can map, fold, filter and
 foreach on them in parallel.
 As all of these functions require input functions to work, the types of
 functions you use are important.
 The docs warn:
\end_layout

\begin_layout Itemize
Side-effecting operations can lead to non-determinism 
\end_layout

\begin_layout Itemize
Non-associative operations lead to non-determinism
\end_layout

\begin_layout Standard
Having experienced something like this myself my advice would be to not
 have references to parallel collections, but rather immediately transform
 them to sequential ones, or at least not pass parallel collections between
 functions.
 It's easy to forget that the collections you are working with are not determini
stic for certain operations.
 Languages like Haskell, where side-effects are more controlled, have an
 advantage here, as side-effecting operations are discouraged.
\end_layout

\begin_layout Paragraph
Hardware implementations
\end_layout

\begin_layout Standard
There are some hardware implementations of parallel statements, like SIMD
 and OpenCL.
 SIMD (Single Instruction, Multiple Data) is a kind of processor instruction
 that allows a processor to do exactly what the name states very efficiently.
 On x86 platforms AVX (Advanced Vector eXtensions) is the name of the instructio
ns for doing this.
\end_layout

\begin_layout Standard
In the same vein, GPUs are very good at taking a lot of floating point numbers
 and doing the same operation on all of them.
 Given that a modern AMD GPU has close to 3000 processing units and runs
 at 1GHz it's given that certain operations are going to be faster on a
 GPU than on a CPU.
 This has had a huge impact on many fields, for example in Artificial Intelligen
ce it was believed that neural networks was inferior to many other techniques.
 Turns out that when you train the network on a GPU you can afford bigger
 neural networks and this improves performance so much that lately this
 has become the best AI technique for many fields.
 If you've heard some hype about 
\begin_inset Quotes eld
\end_inset

deep learning
\begin_inset Quotes erd
\end_inset

, that's basically just big neural networks made possible by GPUs and some
 algorithmic insights that I don't know particularly much about.
 There are two ways to interface with GPUs: OpenCL and CUDA, of which CUDA
 is only for Nvidia cards.
\end_layout

\begin_layout Section
Actors, Futures and STM, Together.
 
\begin_inset Index idx
status open

\begin_layout Plain Layout
Actors
\end_layout

\end_inset


\begin_inset Index idx
status open

\begin_layout Plain Layout
Futures
\end_layout

\end_inset


\begin_inset Index idx
status open

\begin_layout Plain Layout
STM
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "sec:Actors,-Futures-and"

\end_inset


\end_layout

\begin_layout Standard
In Akka, futures are used by default to avoid blocking when waiting for
 a response, effectively allowing an actor to process several messages concurren
tly, if it needs to.
 This dramatically increases the complexity required to produce a deadlock,
 and does away with the classical 
\begin_inset Quotes eld
\end_inset

A waits on B and B waits on A
\begin_inset Quotes erd
\end_inset

 example entirely.
 For instance this program...
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "breaklines=true,frame=lrtb,language=scala,numbers=left,showstringspaces=false"
inline false
status open

\begin_layout Plain Layout

import akka.actor.Actor
\end_layout

\begin_layout Plain Layout

import akka.actor.ActorRef
\end_layout

\begin_layout Plain Layout

import akka.actor.ActorSystem
\end_layout

\begin_layout Plain Layout

import akka.actor.Props
\end_layout

\begin_layout Plain Layout

import akka.agent.Agent
\end_layout

\begin_layout Plain Layout

import scala.sys
\end_layout

\begin_layout Plain Layout

import java.lang.Thread
\end_layout

\begin_layout Plain Layout

import scala.collection.mutable.ArrayBuffer
\end_layout

\begin_layout Plain Layout

import scala.util.Random
\end_layout

\begin_layout Plain Layout

import akka.util.Timeout
\end_layout

\begin_layout Plain Layout

import scala.concurrent._
\end_layout

\begin_layout Plain Layout

import scala.concurrent.ExecutionContext.Implicits.global
\end_layout

\begin_layout Plain Layout

import scala.language.postfixOps
\end_layout

\begin_layout Plain Layout

import akka.pattern.ask
\end_layout

\begin_layout Plain Layout

import scala.util.{Failure, Success}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

class FutureSTMActor(printstr:String, agent:Agent[String]) extends Actor
 {
\end_layout

\begin_layout Plain Layout

  var othermessagesanswered:Int = 0
\end_layout

\begin_layout Plain Layout

  def receive = 
\end_layout

\begin_layout Plain Layout

  {
\end_layout

\begin_layout Plain Layout

    case other: ActorRef =>
\end_layout

\begin_layout Plain Layout

    {
\end_layout

\begin_layout Plain Layout

      val response = other.ask("please reply")(50000) //the 50000 here is
 the timeout.
 It's required and I just chose a very large value
\end_layout

\begin_layout Plain Layout

      response onComplete 							//executed when response arrives or timeout
\end_layout

\begin_layout Plain Layout

      { 
\end_layout

\begin_layout Plain Layout

        case Success(result) => println("success: " + result); println(printstr
 + ": " + "STM Int says main thread in iteration: " + agent.get)
\end_layout

\begin_layout Plain Layout

        case Failure(failure) => println(failure)
\end_layout

\begin_layout Plain Layout

      }
\end_layout

\begin_layout Plain Layout

      othermessagesanswered += 1
\end_layout

\begin_layout Plain Layout

      println(printstr + ": " + othermessagesanswered.toString())
\end_layout

\begin_layout Plain Layout

    }
\end_layout

\begin_layout Plain Layout

    case "please reply" =>
\end_layout

\begin_layout Plain Layout

    {
\end_layout

\begin_layout Plain Layout

      sender() ! "this is a reply"
\end_layout

\begin_layout Plain Layout

    }
\end_layout

\begin_layout Plain Layout

    case s: String =>
\end_layout

\begin_layout Plain Layout

    {
\end_layout

\begin_layout Plain Layout

      println("got string: " + s)
\end_layout

\begin_layout Plain Layout

    }
\end_layout

\begin_layout Plain Layout

  }
\end_layout

\begin_layout Plain Layout

}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

object Main extends App  
\end_layout

\begin_layout Plain Layout

{
\end_layout

\begin_layout Plain Layout

	val agent = Agent("0"*10)
\end_layout

\begin_layout Plain Layout

	val system = ActorSystem("DemoSystem")
\end_layout

\begin_layout Plain Layout

    val aActor = system.actorOf(Props(classOf[FutureSTMActor], "a", agent))
\end_layout

\begin_layout Plain Layout

    val bActor = system.actorOf(Props(classOf[FutureSTMActor], "b", agent))
\end_layout

\begin_layout Plain Layout

    for (x <- 0 to 10)
\end_layout

\begin_layout Plain Layout

    {
\end_layout

\begin_layout Plain Layout

      println("m: " + x.toString)
\end_layout

\begin_layout Plain Layout

      agent send ((x.toString) * 10)
\end_layout

\begin_layout Plain Layout

      println("m: " + agent.get.toString)
\end_layout

\begin_layout Plain Layout

      Future{ aActor ! bActor }
\end_layout

\begin_layout Plain Layout

      Future{ bActor ! aActor }
\end_layout

\begin_layout Plain Layout

      if(x%1==0) { Thread.sleep(2) }
\end_layout

\begin_layout Plain Layout

    }
\end_layout

\begin_layout Plain Layout

    Thread.sleep(1000)
\end_layout

\begin_layout Plain Layout

    scala.sys.exit()
\end_layout

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
...
 will not deadlock.
 Moreover, the two actors will not even be in synchrony.
 Since they wait for each other with futures, they're not really waiting
 at all!
\end_layout

\begin_layout Standard
Now, you think we may be covered, but the problem with messages is one of
 synchronization.
 How do you make sure that an entire system of actors all have the same
 value, for example money in your bank account? One way to do it is to use
 STM, as we here do, synchronizing the current iteration number in the main
 thread's loop across actors using Agents, which are inspired by Clojure's
 STM Agents.
 As expected, the STM Agent is synchronized through the entire run, even
 though nothing else among the agents are.
\end_layout

\begin_layout Standard
An interesting problem occured when I tested this.
 Sometimes the number in the shared string would go downwards.
 Why? Turns out sometimes the transactions are enacted out of order, and
 the STM variable is set to a late value after a newer one.
 This isn't a feature that's universal to STM, but rather this particular
 implementation; usually updates from a single thread arrive in-order.
 A way to get around this is to increment the value instead of assigning
 to it, like so: 
\begin_inset listings
lstparams "frame=lrtb,numbers=left"
inline false
status open

\begin_layout Plain Layout

agent send (_ + 1)
\end_layout

\begin_layout Plain Layout

//or, if you want to be consistent in your anonymous function syntax:
\end_layout

\begin_layout Plain Layout

agent send (x=>x+1)
\end_layout

\end_inset

This is an important reminder that even STM does not let you write concurrent
 code as if it was single-threaded.
 
\end_layout

\begin_layout Section
Scheduling Lightweight Threads
\begin_inset Index idx
status open

\begin_layout Plain Layout
Scheduling
\end_layout

\end_inset


\end_layout

\begin_layout Standard
If you are goin to run N lightweight threads on M OS threads, you'll have
 to spread the tasks over, or dispatch them to, the OS threads somehow.
 What I'm saying is that you have to write a scheduler.
 As someone who has done that while writing this: Oy vey, woe be you.
 It's not a particularly easy task, but some smart people have thought about
 it before you and I have:
\end_layout

\begin_layout Subsection
Thread-pool dispatching
\begin_inset Index idx
status open

\begin_layout Plain Layout
thread-pool dispatching
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The tasks to be run are divided into groups of assumed equal load and given
 to the real threads for execution.
 This method is divided into first a divide and then a conquer phase.
 If one group of tasks turns out to take longer than another the real thread
 with the easier task group is left idling.
\end_layout

\begin_layout Subsection
Fork-join dispatching
\begin_inset Index idx
status open

\begin_layout Plain Layout
fork-join dispatching
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Juicy stuff.
 This is what Go and Akka uses by default, and with good reason.
 In actor or channel concurrency most lightweight threads sit around waiting
 for messages at any given time.
 That's fine as long as the lightweight threads doing work are spread out
 roughly equally over the OS threads.
 But what if they aren't? What if all the lightweight threads doing work
 at any given time are on a single OS thread? Well, then you've written
 a multithreaded program where a only one thread at a time does any work.
 Fortunately, this dispatching method fixes the problem when applied correctly.
 So read on.
\end_layout

\begin_layout Standard
Fork-join is similar to thread-pool, but uses something called work stealing.
 With work stealing, a thread that finishes its tasks early can 
\begin_inset Quotes eld
\end_inset

steal
\begin_inset Quotes erd
\end_inset

 tasks from a thread that's still busy.To avoid overworking the scheduling
 problem tasks are grouped in groups, which makes the following thread a
 bit hard to understand.
 What happens is that the tasks are grouped in a single group and pushed
 on a stack.
 Each real thread can then pop a group of tasks and either split the group
 and push the resulting groups or execute the group.
 In general, real threads will only execute very small groups, splitting
 groups instead if they're too large.
 Task groups should be considered small enough when the expected overhead
 of worrying about their size is larger than the expected cost of a single
 group being too large.
 The benefit of this method over thread-pool is that the divide and conquer
 phases are interleaved.
 There are a lot more task groups than threads, so a misjudgment of the
 complexity of a single task group should have smaller consequence.
\end_layout

\begin_layout Standard
The 
\begin_inset Quotes eld
\end_inset

work stealing
\begin_inset Quotes erd
\end_inset


\begin_inset Index idx
status open

\begin_layout Plain Layout
work stealing
\end_layout

\end_inset

 term comes from an alternative implementation where the N real threads
 split the tasks into N task groups which each real thread finally splits
 and places on its own work stack.
 Once a real thread is done with its own task groups it will try to steal
 task groups from other real thread's stacks.
 If there's no work to find there it will look to an input queue of work
 common to all N real threads.
\end_layout

\begin_layout Standard
The first variant is handy if the tasks are messages to be processed, but
 if the tasks are the lightweight threads then you need the work stealing
 from the second method.
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
The popular approaches taken to concurrency in modern languages can be roughly
 divided into three: The actor model, channels and software transactional
 memory.
 The three approaches come from different mindsets and priorities.
 Users of the actor model often use it to create distributed, parallel,
 fault-tolerant systems.
 Users of CSP often want lower-level concurrency mechanisms that are easier
 to reason about and more predictable.
 STM and parallel statements work neatly with both paradigms.
\end_layout

\begin_layout Paragraph
The differences between CSP and actor model
\end_layout

\begin_layout Itemize
CSP threads are anonymous, actors are named.
\end_layout

\begin_layout Itemize
CSP communication is synchronous, actor communication is asynchronous.
\end_layout

\begin_layout Itemize
CSP communication is multi-sender, multi-receiver, actor communication is
 one-to-one.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Part
Proposal
\end_layout

\begin_layout Standard
A recurring theme in discussing these topics online seems to be a need for
 predictability.
 POSIX threads are not predictable, but are or may be perceived as more
 predictable than lightweight threads with lots of stuff happening underneath
 in a non-obvious manner.
\end_layout

\begin_layout Standard
I propose an implementation that is as simple and deterministic as possible,
 providing guarantees like 
\begin_inset Quotes eld
\end_inset

all threads get to run a decent amout
\begin_inset Quotes erd
\end_inset

.
 Exposed functionality should be something like:
\end_layout

\begin_layout Enumerate
Parmap, a very simple mapping of one array to another of type 
\begin_inset Formula $arrayA[x]=f(arrayB[x])$
\end_inset

 for all elements x.
 Executed in parallel.
\end_layout

\begin_layout Enumerate
Futures, parallel computation
\end_layout

\begin_layout Enumerate
Lightweight threads with channels.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newpage
\end_layout

\end_inset


\end_layout

\begin_layout Part
Prototype
\end_layout

\begin_layout Standard
A prototype for parmap can be seen in 
\begin_inset CommandInset ref
LatexCommand ref
reference "C parmap"

\end_inset

.
 We'll now look at the prototype for lightweight threads.
 I haven't made a prototype for futures.
\end_layout

\begin_layout Section
Scheduler
\end_layout

\begin_layout Standard
Of paramount importance in implementing lightweight threads is the scheduler.
 Both Go and Akka use a work-stealing scheduler and this seems to be the
 industry standard.
 Because it's simple and I'm not very good at C programming I made a simpler
 scheduler in which all worker threads compete for all lightweight threads
 all the time.
 For lightweights that don't do much the result is catastrophic as worker
 threads compete for mutexes:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename /home/tormod/Documents/skole/fall2014/uniproject/doc/lightLoadedLightThreadsDumbSched.svg
	lyxscale 70
	width 40text%

\end_inset


\end_layout

\begin_layout Standard
For lightweight threads that do a fair amount of work the result is a lot
 better:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename /home/tormod/Documents/skole/fall2014/uniproject/doc/hevyLoadedLightThreadsDumbSched.svg
	lyxscale 70
	width 40text%

\end_inset


\end_layout

\begin_layout Standard
Note that the time given on the y axis can not be compared across graphs,
 as the amount of repetitions is different.
 There are three lightweight threads, two of which sends messages to each
 other.
 That's why we speedup when moving from two to three workers is so lacklustre
 and performance only improves until we have three threads.
 The perfect amount of worker threads seems best determined by the programmer,
 who knows how the light threads will communicate, and thus what the real
 amount of paralellism is.
 That said, thread-pooling or work-stealing schedulers deal close to optimally
 with surplus threads.
\end_layout

\begin_layout Standard
An important point can be observed from this: You can make your code slower
 by throwing more threads at it! Even a good scheduler still has a lot of
 overhead and that overhead may take more time than the actual work, if
 that amount of work is small enough.
\end_layout

\begin_layout Paragraph
Smarter scheduling with thread-pool dispatching
\end_layout

\begin_layout Standard
This is the final scheduler with very light load on the lightweight threads:
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename lightLoadedLightThreadsSmartSched.svg
	lyxscale 70
	width 40text%

\end_inset


\end_layout

\begin_layout Standard
Again, I wouldn't compare seconds across graphs, because I have been very
 incosistent about number of iterations etc.
 Unfortunately, the final scheduler is unable to cope with more than one
 OS thread per lightweight thread, so that's why the threads don't go to
 seven here.
 Fortunately that situation makes absolutely no sense for thread-pool schedulers
, as the superfluous threads would have nothing to do.
 In the discussion about fork-join dispatching I mentioned that it solved
 the problem of bad labor division among the OS threads.
 Here we can see such a problem play out as time goes down from two threads
 to three.
 In lightweight threads that do almost no work the majority of the time
 is spent in the scheduler.
 It's therefore better to just have as few threads as possible to make the
 scheduling go as fast as possible.
 The only reason I can see for the performance going up as we move from
 two to three is that the lightweight threads are badly handed out.
 If you're looking at the source right now, I'm thinking the grouping is
 [[foo], [baz, lol]] when a better grouping would be [[foo, baz], [lol]]
 as lol is independent of foo and baz and the latter two cannot run at the
 same time.
 With a work-stealing scheduler this would have solved itself pretty quickly.
\end_layout

\begin_layout Standard
From two to three threads we also see mutex contention being a much lesser
 problem than earlier.
 It's basically gone, in fact, as this is the goal of thread-pool scheduling.
\end_layout

\begin_layout Section
The Structure of the Prototype.
 Or, a brief Documentation
\end_layout

\begin_layout Standard
All debug printouts left intact so you can see what's going on more easily.
\end_layout

\begin_layout Standard
The prototype has 3 coroutines that prints messages and otherwise yield,
 resume etc.
 Their functionality is held in the functions printfoo, printbaz and printlol.
 They all use a Duff's Device-inspired switch-case to handle yield and resume.
\end_layout

\begin_layout Itemize
printfoo prints foo and sends messages to printbaz
\end_layout

\begin_layout Itemize
printbaz prints baz and receives messages from foo
\end_layout

\begin_layout Itemize
printlol prints lol and goes in a loop, randomly yielding.
\end_layout

\begin_layout Standard
The coroutines have 7 functions to handle bookkeeping when yielding.
 These are retsend, retrecv, rettryrecv, retcont, retloop, retfin and retsleep.
 All, like retcont, do continuation bookkeeping, except retfin, which doesn't
 need to.
\end_layout

\begin_layout Itemize
retsend yields and instructs the scheduler to place a specified message
 on the specified channel.
 Scheduler resumes coroutine when message sent.
\end_layout

\begin_layout Itemize
retrecv yields and instructs the scheduler to resume when a message has
 been received on the specified channel; blocking receive.
\end_layout

\begin_layout Itemize
rettryrecv yields and instructs the scheduler to resume the coroutine, receiving
 a message on the specified channel if one is available; non-blocking receive.
\end_layout

\begin_layout Itemize
retcont yields and instructs the scheduler to resume at the specified continuati
on.
\end_layout

\begin_layout Itemize
retloop yields and instructs the scheduler to resume at the start of the
 function the coroutine is executing.
\end_layout

\begin_layout Itemize
retfin yields and instructs the scheduler to never resume.
\end_layout

\begin_layout Itemize
retsleep yields and instructs the scheduler to only resume after a specified
 amount of time.
\end_layout

\begin_layout Standard
rettryrecv and retsleep are untested but may work.
 They're mostly there for illustration purposes.
\end_layout

\begin_layout Standard
There are five structs:
\end_layout

\begin_layout Itemize
Datastruct holds the data the routines do work on.
 One per coroutine.
\end_layout

\begin_layout Itemize
Comstruct holds communication information; what channel the thread is communicat
ing over and the message it is sending or receiving.
 One per coroutine.
\end_layout

\begin_layout Itemize
Sysstruct holds instructions for the scheduler, like whether the coroutine
 is sending, receiving or is ready for continuation and the place it is
 ready for continuation.
 One per coroutine.
\end_layout

\begin_layout Itemize
ThreadArgument holds the POSIX thread creation arguments.
 It houses mutexes and pointers to all other structs as well as the threadid
 and the amount of coroutines the thread is responsible for.
 One per POSIX thread.
\end_layout

\begin_layout Itemize
Channel holds messages and state of a channel.
 One per channel.
\end_layout

\begin_layout Standard
What remains are three functions: scheduler, coroutines and main.
\end_layout

\begin_layout Itemize
scheduler is the scheduler.
 It's a huge mess, but then again, it's a 
\emph on
scheduler
\emph default
.
 The POSIX thread returns to this function when a coroutine yields.
 It then determines what to do next.
 Passing messages over channels is also done here in order to keep the mutexes
 out of user functions.
 The grand theme of what happens in the scheduler is that the OS thread
 goes over its coroutines in a loop, treating them according to their waitstate.
 If a thread is seen as hogging or blocking shared resources it will usleep
 a bit so as to not do exactly that.
 
\end_layout

\begin_layout Itemize
coroutines stuff away the work of spawning the POSIX threads and everything
 related.
 It builds the ThreadArguments and creates the mutexes.
\end_layout

\begin_layout Itemize
main is a mix of what the user of the prototype should do and what a user
 should definitely not have to do but has to do anyway.
\end_layout

\begin_layout Standard
Nowhere in the prototype is memory ever deallocated.
 Since memory isn't being allocated over the runtime of a program I don't
 see this as a serious problem.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "llnl.gov pthreads"
key "llnl.gov pthreads"

\end_inset

https://computing.llnl.gov/tutorials/pthreads/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "Haskell STM Example"
key "Haskell STM Example"

\end_inset

https://www.haskell.org/haskellwiki/Simple_STM_example
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "James Mickens: The Night Watch"
key "Mickens:NW"

\end_inset

http://research.microsoft.com/en-us/people/mickens/thenightwatch.pdf
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "Akka actorRefs"
key "Akka actorRefs"

\end_inset

http://doc.akka.io/docs/akka/snapshot/general/addressing.html
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "Scala parcollections"
key "Scala parcollections"

\end_inset

http://docs.scala-lang.org/overviews/parallel-collections/overview.html
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "Rust heavyweight threads"
key "Rust heavyweight threads"

\end_inset

https://mail.mozilla.org/pipermail/rust-dev/2013-December/007565.html
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "LabView parallelism"
key "LabView parallelism"

\end_inset

Labhttp://www.ni.com/white-paper/6099/en/
\end_layout

\begin_layout Standard
\begin_inset CommandInset index_print
LatexCommand printindex
type "idx"

\end_inset


\end_layout

\end_body
\end_document
